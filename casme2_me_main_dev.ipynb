{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A jupyter notebook version file for the `main.py`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `autoreload` to execute the change in `.py` files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_path: D:\\Databases\\CAS(ME)^2\\preds\\me_sl_swin_t_batch_size_48_epochs_25_pseudo_labeling_128_2.pkl\n"
     ]
    }
   ],
   "source": [
    "import _pickle\n",
    "import natsort\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "dataset_dir = \"D:/Databases/CAS(ME)^2\"\n",
    "# dataset_dir = \"F:/HEH/Databases/CAS(ME)^2\"\n",
    "# dataset_dir = \"/data/disk1/heh/databases/CAS(ME)^2\"\n",
    "\n",
    "images_loading = False\n",
    "image_size = 128\n",
    "load_cropped_images = False\n",
    "# expression_type = \"mae\"  # macro-expression spotting\n",
    "expression_type = \"me\"  # micro-expression spotting\n",
    "save_x = False\n",
    "debug_preds = True\n",
    "labeling_function = \"pseudo_labeling\"\n",
    "# labeling_function = \"original_labeling\"\n",
    "model_names = {\n",
    "    0: \"SOFTNet\",\n",
    "    1: \"SOFTNetCBAM\",\n",
    "    2: \"ViT-B\",\n",
    "    3: \"SL-ViT-B\",\n",
    "    4: \"Swin-T\",\n",
    "    5: \"Swin-S\",\n",
    "    6: \"L-Swin-T\",\n",
    "    7: \"S-Swin-T\",\n",
    "    8: \"SL-Swin-T\",\n",
    "    9: \"SL-Swin-S\",\n",
    "}\n",
    "model_name = model_names[8]\n",
    "batch_size = 48\n",
    "epochs = 25\n",
    "save_preds = False\n",
    "preds_stem = (\n",
    "    f\"{expression_type}_\"\n",
    "    + model_name.lower().replace(\"-\", \"_\")\n",
    "    + f\"_batch_size_{batch_size}\"\n",
    "    + f\"_epochs_{epochs}\"\n",
    "    + f\"_{labeling_function}\"\n",
    "    + f\"_{image_size}\"\n",
    "    + \"_2\"\n",
    ")\n",
    "preds_path = Path(dataset_dir, \"preds\", preds_stem).with_suffix(\".pkl\")\n",
    "print(f\"preds_path: {preds_path}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing import *\n",
    "\n",
    "# crop_images_dev(dataset_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debug the image processing, the videos_images is from cropped_rawpic, whereas the other variables are from rawpic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  s15\n",
      "subject:  s16\n",
      "subject:  s19\n",
      "subject:  s20\n",
      "subject:  s21\n",
      "subject:  s22\n",
      "subject:  s23\n",
      "subject:  s24\n",
      "subject:  s25\n",
      "subject:  s26\n",
      "subject:  s27\n",
      "subject:  s29\n",
      "subject:  s30\n",
      "subject:  s31\n",
      "subject:  s32\n",
      "subject:  s33\n",
      "subject:  s34\n",
      "subject:  s35\n",
      "subject:  s36\n",
      "subject:  s37\n",
      "subject:  s38\n",
      "subject:  s40\n"
     ]
    }
   ],
   "source": [
    "from image_processing import *\n",
    "\n",
    "# videos_images, subjects, subjects_videos_code = load_images(dataset_dir)\n",
    "\n",
    "videos_images, subjects, subjects_videos_code = load_images_dev(\n",
    "    dataset_dir,\n",
    "    images_loading=images_loading,\n",
    "    image_size=image_size,\n",
    "    load_cropped_images=load_cropped_images,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects: ['s15', 's16', 's19', 's20', 's21', 's22', 's23', 's24', 's25', 's26', 's27', 's29', 's30', 's31', 's32', 's33', 's34', 's35', 's36', 's37', 's38', 's40']\n",
      "subjects_videos_code: [['0101', '0102', '0401', '0402', '0502', '0503', '0505', '0508'], ['0101', '0102', '0401', '0402', '0502', '0505', '0507'], ['0102', '0402', '0505', '0507', '0502'], ['0502'], ['0101', '0401'], ['0101', '0102', '0402', '0503', '0508'], ['0102', '0402', '0503', '0507'], ['0101', '0401', '0402', '0502', '0507'], ['0101', '0102', '0502', '0508'], ['0101', '0102', '0401', '0503'], ['0101', '0102', '0401', '0402', '0502', '0503', '0505', '0507', '0508'], ['0502'], ['0101', '0102', '0401', '0502', '0503', '0505', '0507'], ['0101', '0401', '0402', '0502', '0503', '0505', '0507'], ['0101', '0102', '0401', '0402', '0502', '0503', '0505', '0507', '0508'], ['0102', '0402'], ['0401', '0402', '0503'], ['0102'], ['0401', '0505'], ['0101', '0402', '0502', '0505', '0507', '0508'], ['0502', '0507'], ['0401', '0502', '0503']]\n"
     ]
    }
   ],
   "source": [
    "print(\"subjects:\", subjects)\n",
    "print(\"subjects_videos_code:\", subjects_videos_code)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Excel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_processing import load_excel\n",
    "\n",
    "Excel_data = load_excel(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>video_name_&amp;_expression_number</th>\n",
       "      <th>onset</th>\n",
       "      <th>apex</th>\n",
       "      <th>offset</th>\n",
       "      <th>AUs</th>\n",
       "      <th>extimated_emotion</th>\n",
       "      <th>expression_type</th>\n",
       "      <th>self-reported_emotion</th>\n",
       "      <th>video_name</th>\n",
       "      <th>video_code</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>anger1_1</td>\n",
       "      <td>557</td>\n",
       "      <td>572</td>\n",
       "      <td>608</td>\n",
       "      <td>4+10+14+15</td>\n",
       "      <td>negative</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger1</td>\n",
       "      <td>0401</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>anger1_2</td>\n",
       "      <td>2854</td>\n",
       "      <td>2862</td>\n",
       "      <td>2871</td>\n",
       "      <td>38</td>\n",
       "      <td>others</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>anger1</td>\n",
       "      <td>0401</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>anger2_1</td>\n",
       "      <td>2155</td>\n",
       "      <td>2163</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger2</td>\n",
       "      <td>0402</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>anger2_2</td>\n",
       "      <td>3363</td>\n",
       "      <td>3371</td>\n",
       "      <td>3383</td>\n",
       "      <td>4+7+14</td>\n",
       "      <td>negative</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger2</td>\n",
       "      <td>0402</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>anger2_3</td>\n",
       "      <td>3380</td>\n",
       "      <td>3386</td>\n",
       "      <td>3407</td>\n",
       "      <td>4+14+38</td>\n",
       "      <td>negative</td>\n",
       "      <td>macro-expression</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger2</td>\n",
       "      <td>0402</td>\n",
       "      <td>s15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant video_name_&_expression_number  onset  apex  offset  \\\n",
       "0            1                       anger1_1    557   572     608   \n",
       "1            1                       anger1_2   2854  2862    2871   \n",
       "2            1                       anger2_1   2155  2163       0   \n",
       "3            1                       anger2_2   3363  3371    3383   \n",
       "4            1                       anger2_3   3380  3386    3407   \n",
       "\n",
       "          AUs extimated_emotion   expression_type self-reported_emotion  \\\n",
       "0  4+10+14+15          negative  macro-expression                 anger   \n",
       "1          38            others  macro-expression               sadness   \n",
       "2         NaN          negative  macro-expression                 anger   \n",
       "3      4+7+14          negative  macro-expression                 anger   \n",
       "4     4+14+38          negative  macro-expression                 anger   \n",
       "\n",
       "  video_name video_code subject  \n",
       "0     anger1       0401     s15  \n",
       "1     anger1       0401     s15  \n",
       "2     anger2       0402     s15  \n",
       "3     anger2       0402     s15  \n",
       "4     anger2       0402     s15  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Excel_data.head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required_videos_index:  [1, 4, 8, 9, 12, 13, 14, 16, 28, 33, 36, 37, 38, 45, 46, 47, 49, 50, 52, 54, 55, 57, 62, 64, 67, 71, 73, 74, 77, 83, 87, 91, 93]\n",
      "len(clean_videos_images) = 33\n"
     ]
    }
   ],
   "source": [
    "from label_processing import load_ground_truth_labels\n",
    "\n",
    "\n",
    "(\n",
    "    clean_videos_images,\n",
    "    clean_subjects_videos_code,\n",
    "    clean_subjects,\n",
    "    clean_subjects_videos_ground_truth_labels,\n",
    ") = load_ground_truth_labels(\n",
    "    dataset_dir,\n",
    "    expression_type,\n",
    "    videos_images,\n",
    "    subjects_videos_code,\n",
    "    subjects,\n",
    "    Excel_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(clean_subjects):  14\n",
      "clean_subjects:  ['s15' 's16' 's19' 's23' 's24' 's25' 's27' 's29' 's30' 's31' 's32' 's35'\n",
      " 's37' 's38']\n",
      "len(clean_subjects_videos_code):  14\n",
      "clean_subjects_videos_codes:  [['0102', '0502'], ['0101', '0102', '0502', '0505', '0507'], ['0402'], ['0102'], ['0401', '0507'], ['0101', '0102'], ['0101', '0102', '0401', '0502', '0503', '0507'], ['0502'], ['0101', '0401'], ['0101', '0402', '0505'], ['0401', '0502', '0503', '0508'], ['0102'], ['0402', '0508'], ['0507']]\n",
      "len(clean_subjects_videos_ground_truth_labels):  14\n",
      "clean_subjects_videos_ground_truth_labels:  [[[[698, 706]], [[137, 147]]], [[[551, 564]], [[269, 277]], [[322, 333]], [[395, 406], [1694, 1709], [1879, 1894]], [[1957, 1967], [2284, 2294]]], [[[1926, 1941]]], [[[330, 345], [525, 539], [726, 739]]], [[[607, 620], [962, 976], [1889, 1901], [2180, 2192], [3440, 3452]], [[1835, 1847], [1950, 1964], [3232, 3247]]], [[[112, 126]], [[995, 1007], [1007, 1016], [1017, 1033]]], [[[873, 887]], [[33, 47], [308, 316], [373, 387]], [[351, 364], [368, 381], [1134, 1146], [1973, 1985]], [[612, 627]], [[418, 431]], [[875, 889]]], [[[139, 151]]], [[[1454, 1465]], [[925, 940]]], [[[1420, 1432]], [[1688, 1701], [2189, 2203], [2376, 2388], [3802, 3814]], [[1045, 1058]]], [[[267, 277]], [[310, 323], [1170, 1183]], [[257, 271], [1030, 1043]], [[285, 300]]], [[[99, 112], [362, 370]]], [[[3501, 3513]], [[417, 429]]], [[[2231, 2246]]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"len(clean_subjects): \", len(clean_subjects))\n",
    "print(\"clean_subjects: \", clean_subjects)\n",
    "print(\"len(clean_subjects_videos_code): \", len(clean_subjects_videos_code))\n",
    "print(\"clean_subjects_videos_codes: \", clean_subjects_videos_code)\n",
    "print(\n",
    "    \"len(clean_subjects_videos_ground_truth_labels): \",\n",
    "    len(clean_subjects_videos_ground_truth_labels),\n",
    ")\n",
    "print(\n",
    "    \"clean_subjects_videos_ground_truth_labels: \",\n",
    "    clean_subjects_videos_ground_truth_labels,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k (Half of average length of expression) =  6\n"
     ]
    }
   ],
   "source": [
    "from label_processing import calculate_k\n",
    "\n",
    "k = calculate_k(clean_subjects_videos_ground_truth_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features and Pro-process\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`image_size = 256` takes about 1171 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features_extraction_and_pre_processing import *\n",
    "\n",
    "# extract_features_and_pre_process(\n",
    "#     clean_videos_images,\n",
    "#     k,\n",
    "#     expression_type,\n",
    "#     clean_subjects,\n",
    "#     clean_subjects_videos_code,\n",
    "#     dataset_dir,\n",
    "#     image_size=image_size,\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 260 m.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Optical Flow Features (shape = [128, 128, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features_extraction import extract_features\n",
    "\n",
    "# clean_videos_images_features = extract_features(clean_videos_images, k, image_size=128)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 44 m.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import pre_process\n",
    "\n",
    "# resampled_clean_videos_images_features = pre_process(\n",
    "#     clean_videos_images, clean_videos_images_features, k\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump Resampled Clean Videos Images Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\n",
    "#     Path(\n",
    "#         dataset_dir,\n",
    "#         f\"resampled_clean_videos_images_{expression_type}_features_{image_size}\",\n",
    "#     ).with_suffix(\".pkl\"),\n",
    "#     \"wb\",\n",
    "# ) as pkl_file:\n",
    "#     _pickle.dump(resampled_clean_videos_images_features, pkl_file)\n",
    "#     pkl_file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load **Original** Resampled Clean Videos Images Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\n",
    "#     Path(\n",
    "#         dataset_dir,\n",
    "#         f\"original_resampled_clean_videos_images_{expression_type}_features.pkl\",\n",
    "#     ),\n",
    "#     \"rb\",\n",
    "# ) as pkl_file:\n",
    "#     resampled_clean_videos_images_features = _pickle.load(pkl_file)\n",
    "#     pkl_file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Resampled Clean Videos Images Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_x is not False and debug_preds is not True:\n",
    "    with open(\n",
    "        Path(\n",
    "            dataset_dir,\n",
    "            f\"resampled_clean_videos_images_{expression_type}_features_{image_size}\",\n",
    "        ).with_suffix(\".pkl\"),\n",
    "        \"rb\",\n",
    "    ) as pkl_file:\n",
    "        resampled_clean_videos_images_features = _pickle.load(pkl_file)\n",
    "        pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_x is not False and debug_preds is not True:\n",
    "    print(\n",
    "        \"len(resampled_clean_videos_images_features): \",\n",
    "        len(resampled_clean_videos_images_features),\n",
    "    )\n",
    "    print(\n",
    "        \"len(resampled_clean_videos_images_features[0]): \",\n",
    "        len(resampled_clean_videos_images_features[0]),\n",
    "    )\n",
    "    print(\n",
    "        \"resampled_clean_videos_images_features[0][0].shape: \",\n",
    "        resampled_clean_videos_images_features[0][0].shape,\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 80463\n"
     ]
    }
   ],
   "source": [
    "from labeling import *\n",
    "\n",
    "if labeling_function == \"pseudo_labeling\":\n",
    "    labels = get_pseudo_labels(\n",
    "        clean_videos_images, clean_subjects_videos_ground_truth_labels, k\n",
    "    )\n",
    "elif labeling_function == \"original_labeling\":\n",
    "    labels = get_original_labels(\n",
    "        clean_videos_images, clean_subjects_videos_ground_truth_labels, k\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # legacy_labels = legacy_get_pseudo_labels(\n",
    "# #     clean_videos_images, clean_subjects_videos_ground_truth_labels, k\n",
    "# # )\n",
    "# legacy_labels = legacy_get_original_labels(\n",
    "#     clean_videos_images, clean_subjects_videos_ground_truth_labels, k\n",
    "# )\n",
    "# labels = np.asarray(labels)\n",
    "# legacy_labels = np.asarray(legacy_labels)\n",
    "# print((labels == legacy_labels).all())\n",
    "# for index, (label, legacy_label) in enumerate(zip(labels, legacy_labels)):\n",
    "#     if label != legacy_label:\n",
    "#         print(index, label, legacy_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for LOSO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Index for each subject:-\n",
      "\n",
      "subject s15 ( group = 0): 0 -> 3336\n",
      "subject s15 has 2 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  2\n",
      "\n",
      "subject s16 ( group = 1): 3336 -> 14273\n",
      "subject s16 has 5 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  7\n",
      "\n",
      "subject s19 ( group = 2): 14273 -> 18638\n",
      "subject s19 has 1 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  8\n",
      "\n",
      "subject s23 ( group = 3): 18638 -> 19703\n",
      "subject s23 has 1 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  9\n",
      "\n",
      "subject s24 ( group = 4): 19703 -> 26673\n",
      "subject s24 has 2 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  11\n",
      "\n",
      "subject s25 ( group = 5): 26673 -> 29790\n",
      "subject s25 has 2 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  13\n",
      "\n",
      "subject s27 ( group = 6): 29790 -> 44942\n",
      "subject s27 has 6 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  19\n",
      "\n",
      "subject s29 ( group = 7): 44942 -> 47211\n",
      "subject s29 has 1 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  20\n",
      "\n",
      "subject s30 ( group = 8): 47211 -> 52966\n",
      "subject s30 has 2 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  22\n",
      "\n",
      "subject s31 ( group = 9): 52966 -> 61673\n",
      "subject s31 has 3 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  25\n",
      "\n",
      "subject s32 ( group = 10): 61673 -> 71111\n",
      "subject s32 has 4 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  29\n",
      "\n",
      "subject s35 ( group = 11): 71111 -> 72186\n",
      "subject s35 has 1 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  30\n",
      "\n",
      "subject s37 ( group = 12): 72186 -> 77205\n",
      "subject s37 has 2 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  32\n",
      "\n",
      "subject s38 ( group = 13): 77205 -> 80463\n",
      "subject s38 has 1 clean video(s)\n",
      "sum clean_subject_videos_ground_truth_labels_len:  33\n"
     ]
    }
   ],
   "source": [
    "from loso_preparing import prepare_for_loso_dev\n",
    "\n",
    "if save_x is False:\n",
    "    resampled_clean_videos_images_features = None\n",
    "\n",
    "y, groups = prepare_for_loso_dev(\n",
    "    resampled_clean_videos_images_features,\n",
    "    labels,\n",
    "    clean_subjects,\n",
    "    clean_videos_images,\n",
    "    clean_subjects_videos_ground_truth_labels,\n",
    "    k,\n",
    "    save_x,\n",
    "    expression_type,\n",
    "    dataset_dir,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_preds is False:\n",
    "    from training_dev import train\n",
    "\n",
    "    preds = train(\n",
    "        dataset_dir=dataset_dir,\n",
    "        clean_subjects=clean_subjects,\n",
    "        image_size=image_size,\n",
    "        y=y,\n",
    "        expression_type=expression_type,\n",
    "        model_name=model_name,\n",
    "        train_or_not=True,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        preds_path=preds_path,\n",
    "    )\n",
    "else:\n",
    "    with open(preds_path, \"rb\") as pkl_file:\n",
    "        preds = _pickle.load(pkl_file)\n",
    "        pkl_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_preds is True:\n",
    "    with open(preds_path, \"wb\") as pkl_file:\n",
    "        _pickle.dump(preds, pkl_file)\n",
    "        pkl_file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotting and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1/14 is in process.\n",
      "0 video(s) have been processed.\n",
      "The current video be processed: subject s15, video 0102\n",
      "The current video be processed: subject s15, video 0502\n",
      "\n",
      "True Positive: 0, False Posive: 11, False Negative: 2\n",
      "Precision = 2.220446049250313e-16, Recall =2.220446049250313e-16, F1-Score = 4.440892098500626e-16\n",
      "Split 1/14 is processed.\n",
      "\n",
      "Split 2/14 is in process.\n",
      "2 video(s) have been processed.\n",
      "The current video be processed: subject s16, video 0101\n",
      "The current video be processed: subject s16, video 0102\n",
      "The current video be processed: subject s16, video 0502\n",
      "The current video be processed: subject s16, video 0505\n",
      "The current video be processed: subject s16, video 0507\n",
      "\n",
      "True Positive: 1, False Posive: 42, False Negative: 9\n",
      "Precision = 0.023255813953488594, Recall =0.10000000000000023, F1-Score = 0.0377358490566043\n",
      "Split 2/14 is processed.\n",
      "\n",
      "Split 3/14 is in process.\n",
      "7 video(s) have been processed.\n",
      "The current video be processed: subject s19, video 0402\n",
      "\n",
      "True Positive: 1, False Posive: 47, False Negative: 10\n",
      "Precision = 0.020833333333333554, Recall =0.09090909090909113, F1-Score = 0.0338983050847463\n",
      "Split 3/14 is processed.\n",
      "\n",
      "Split 4/14 is in process.\n",
      "8 video(s) have been processed.\n",
      "The current video be processed: subject s23, video 0102\n",
      "\n",
      "True Positive: 1, False Posive: 67, False Negative: 13\n",
      "Precision = 0.014705882352941398, Recall =0.07142857142857165, F1-Score = 0.024390243902439563\n",
      "Split 4/14 is processed.\n",
      "\n",
      "Split 5/14 is in process.\n",
      "9 video(s) have been processed.\n",
      "The current video be processed: subject s24, video 0401\n",
      "The current video be processed: subject s24, video 0507\n",
      "\n",
      "True Positive: 2, False Posive: 73, False Negative: 20\n",
      "Precision = 0.02666666666666689, Recall =0.09090909090909113, F1-Score = 0.04123711340206237\n",
      "Split 5/14 is processed.\n",
      "\n",
      "Split 6/14 is in process.\n",
      "11 video(s) have been processed.\n",
      "The current video be processed: subject s25, video 0101\n",
      "The current video be processed: subject s25, video 0102\n",
      "\n",
      "True Positive: 4, False Posive: 82, False Negative: 22\n",
      "Precision = 0.046511627906976966, Recall =0.15384615384615408, F1-Score = 0.07142857142857194\n",
      "Split 6/14 is processed.\n",
      "\n",
      "Split 7/14 is in process.\n",
      "13 video(s) have been processed.\n",
      "The current video be processed: subject s27, video 0101\n",
      "The current video be processed: subject s27, video 0102\n",
      "The current video be processed: subject s27, video 0401\n",
      "The current video be processed: subject s27, video 0502\n",
      "The current video be processed: subject s27, video 0503\n",
      "The current video be processed: subject s27, video 0507\n",
      "\n",
      "True Positive: 8, False Posive: 103, False Negative: 29\n",
      "Precision = 0.0720720720720723, Recall =0.21621621621621645, F1-Score = 0.10810810810810861\n",
      "Split 7/14 is processed.\n",
      "\n",
      "Split 8/14 is in process.\n",
      "19 video(s) have been processed.\n",
      "The current video be processed: subject s29, video 0502\n",
      "\n",
      "True Positive: 8, False Posive: 118, False Negative: 30\n",
      "Precision = 0.06349206349206371, Recall =0.2105263157894739, F1-Score = 0.0975609756097566\n",
      "Split 8/14 is processed.\n",
      "\n",
      "Split 9/14 is in process.\n",
      "20 video(s) have been processed.\n",
      "The current video be processed: subject s30, video 0101\n",
      "The current video be processed: subject s30, video 0401\n",
      "\n",
      "True Positive: 9, False Posive: 151, False Negative: 31\n",
      "Precision = 0.05625000000000022, Recall =0.22500000000000023, F1-Score = 0.09000000000000052\n",
      "Split 9/14 is processed.\n",
      "\n",
      "Split 10/14 is in process.\n",
      "22 video(s) have been processed.\n",
      "The current video be processed: subject s31, video 0101\n",
      "The current video be processed: subject s31, video 0402\n",
      "The current video be processed: subject s31, video 0505\n",
      "\n",
      "True Positive: 10, False Posive: 162, False Negative: 36\n",
      "Precision = 0.05813953488372115, Recall =0.2173913043478263, F1-Score = 0.09174311926605555\n",
      "Split 10/14 is processed.\n",
      "\n",
      "Split 11/14 is in process.\n",
      "25 video(s) have been processed.\n",
      "The current video be processed: subject s32, video 0401\n",
      "The current video be processed: subject s32, video 0502\n",
      "The current video be processed: subject s32, video 0503\n",
      "The current video be processed: subject s32, video 0508\n",
      "\n",
      "True Positive: 12, False Posive: 190, False Negative: 40\n",
      "Precision = 0.059405940594059625, Recall =0.230769230769231, F1-Score = 0.09448818897637848\n",
      "Split 11/14 is processed.\n",
      "\n",
      "Split 12/14 is in process.\n",
      "29 video(s) have been processed.\n",
      "The current video be processed: subject s35, video 0102\n",
      "\n",
      "True Positive: 12, False Posive: 192, False Negative: 42\n",
      "Precision = 0.05882352941176493, Recall =0.22222222222222243, F1-Score = 0.093023255813954\n",
      "Split 12/14 is processed.\n",
      "\n",
      "Split 13/14 is in process.\n",
      "30 video(s) have been processed.\n",
      "The current video be processed: subject s37, video 0402\n",
      "The current video be processed: subject s37, video 0508\n",
      "\n",
      "True Positive: 12, False Posive: 214, False Negative: 44\n",
      "Precision = 0.053097345132743584, Recall =0.2142857142857145, F1-Score = 0.08510638297872393\n",
      "Split 13/14 is processed.\n",
      "\n",
      "Split 14/14 is in process.\n",
      "32 video(s) have been processed.\n",
      "The current video be processed: subject s38, video 0507\n",
      "\n",
      "True Positive: 12, False Posive: 220, False Negative: 45\n",
      "Precision = 0.051724137931034704, Recall =0.2105263157894739, F1-Score = 0.08304498269896246\n",
      "Split 14/14 is processed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "\n",
    "metric_fn, result_dict = spot_and_evaluate(\n",
    "    preds,\n",
    "    clean_subjects_videos_ground_truth_labels,\n",
    "    clean_videos_images,\n",
    "    clean_subjects,\n",
    "    clean_subjects_videos_code,\n",
    "    k,\n",
    "    p=0.60,\n",
    "    show_plot_or_not=False,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 12, False Posive: 220, False Negative: 45\n",
      "Final Precision = 0.051724137931034704,\n",
      "Final Recall =0.2105263157894739,\n",
      "Final F1-Score = 0.08304498269896246\n",
      "\n",
      "Highest Precision = 0.0720720720720723,\n",
      "Highest Recall =0.230769230769231,\n",
      "Highest F1-Score = 0.10810810810810861\n"
     ]
    }
   ],
   "source": [
    "from evaluation import final_evaluate\n",
    "\n",
    "final_evaluate(metric_fn, result_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters | Value | Value\n",
    "| --- | --- | ---\n",
    "| model | SL-Swin-T | SL-Swin-T\n",
    "| epochs | 20 (331 m) | 25 (775 m)\n",
    "| batch_size | 48 | 96\n",
    "| p | 0.60 | 0.60 | \n",
    "| True Positive | 0 | 13 | \n",
    "| False Positive | 0 | 339 | \n",
    "| False Negative | 0 | 44 | \n",
    "| Precision | 0. | 0.00369 | \n",
    "| Recall | 0. | 0.2280 | \n",
    "| F1-Score | 0.1000 | 0.0635 |  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters | Value | Value | Value\n",
    "| --- | --- | --- | ---\n",
    "| model | SL-Swin-T | SL-Swin-T | SL-Swin-T\n",
    "| epochs | 20 (505 m) | 25 (593 m) | 30 (695 m)\n",
    "| batch_size | 32 | 32 | 32\n",
    "| p | 0.86 | 0.77 | | 0.59\n",
    "| True Positive | 7 | 5 | \n",
    "| False Positive | 51 | 88 | \n",
    "| False Negative | 50 | 52 | \n",
    "| Precision | 0.1207 | 0.0537 | \n",
    "| Recall | 0.1228 | 0.0877 | \n",
    "| F1-Score | 0.1217 | 0.0666 |  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters | Value | Value | Value\n",
    "| --- | --- | --- | ---\n",
    "| model | SL-Swin-T | SL-Swin-T | SL-Swin-T\n",
    "| epochs | 20 (311 m) | 20 (311 m) | 30 (410 m)\n",
    "| batch_size | 48 | 48 | 48\n",
    "| p | 0.57 | 0.59 | 0.55\n",
    "| True Positive | 15 | 15 | 13\n",
    "| False Positive | 244 | 221 | 289\n",
    "| False Negative | 42 | 42 | 44\n",
    "| Precision | 0.0579 | 0.0636 | 0.0430\n",
    "| Recall | 0.2632 | 0.2632 | 0.2280\n",
    "| F1-Score | 0.0949 | 0.1024 | 0.0724\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters | Value | Value | Value | Value\n",
    "| --- | --- | --- | --- | ---\n",
    "| model | SL-Swin-T | SL-Swin-T | SL-Swin-T_2 | SL-Swin-T_3\n",
    "| epochs | 25 (325 m) | 25 (325 m) | 25 (325 m) | 25 (325 m)\n",
    "| batch_size | 48 | 48 | 48 | 48\n",
    "| p | 0.57 | 0.59 | 0.60 | 0.60\n",
    "| True Positive | 14 | 14 | 13 | 12\n",
    "| False Positive | 266 | 239 | 226 | 204\n",
    "| False Negative | 43 | 43 | 44 | 45\n",
    "| Precision | 0.0500 | 0.0553 | 0.0544 | 0.0556\n",
    "| Recall | 0.2456 | 0.2456 | 0.2281 | 0.2105\n",
    "| F1-Score | 0.0830 | 0.0903 | 0.0878 | 0.0879\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters | Value | Value | Value | Value | Value\n",
    "| --- | --- | --- | --- | --- | ---\n",
    "| model | ViT-B | SL-ViT-B | Swin-T | L-Swin-T | S-Swin-T\n",
    "| epochs | 25 ( m) | 25 ( m) | 25 ( m) | 25 (568 m) | 25 (364 m)\n",
    "| batch_size | 96 | 96 | 48 | 48 | 48\n",
    "| p | 0. | 0. | 0.57 | 0.51 | 0.49 | \n",
    "| True Positive | 0 | 0 | 13 | 10 | 16 | \n",
    "| False Positive | 0 | 0 | 262 | 339 | 354 | \n",
    "| False Negative | 0 | 0 | 44 | 47 | 41 | \n",
    "| Precision | 0. | 0 | 0.0473 | 0.0287 | 0.0432 | \n",
    "| Recall | 0. | 0 | 0.2281 | 0.1754 | 0.2807 | \n",
    "| F1-Score | 0. | 0 | 0.0783 | 0.0493 | 0.0749 | \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Labeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters | Value | Value |\n",
    "| --- | --- | --- |\n",
    "| model | Swin-T | SL-Swin-T |\n",
    "| epochs | 25 | 25 |\n",
    "| batch_size | 48 | 48 |\n",
    "| p | 0.58 | 0.57 |\n",
    "| True Positive | 8 | 15 |\n",
    "| False Positive | 237 | 305 |\n",
    "| False Negative | 49 | 42 |\n",
    "| Precision | 0.0327 | 0.0469 |\n",
    "| Recall | 0.1404 | 0.2632 |\n",
    "| F1-Score | 0.0503 | 0.0796 |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretched (Deprecated)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters | Value | Value | Value (tf) | Value (tf) | Value | Value\n",
    "| --- | --- | --- | --- | --- | --- | ---\n",
    "| model | 3D-CNN | SOFTNet | SOFTNet | SOFTNet (dev) | SOFTNet | SOFTNet\n",
    "| epochs | | 10 | 20 (39 m)| 20 (39 m) | 35 (54 m) | 20 (28 m)\n",
    "| batch_size | | 48 | 48 | 48 | 48 | 48\n",
    "| learning_rate | | 0.0005 | 0.0005 | 0.0005 | 0.0005 | 0.0005\n",
    "| True Positive | | 20 | 19 | 16 | 20 | 20\n",
    "| False Positive | | 264 | 252 | 238 | 285 | 409\n",
    "| False Negative | | 37 | 38 | 41 | 37 | 37\n",
    "| Precision | | 0.0704 | 0.0701 | 0.0629 | 0.0655 | 0.0466\n",
    "| Recall | | 0.3509 | 0.3333 | 0.2807 | 0.3508 | 0.3508\n",
    "| F1-Score | 0.0714 | 0.1173 | 0.1159 | 0.1028 | 0.1104 | 0.0823\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without pos_embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters | Value 1 | Value 2 | Value 3 | Value 4 | Value 5 | Value 6 | Value 7\n",
    "| --- | --- | --- | --- | --- | --- | --- | ---\n",
    "| model | SL-Swin-T | SL-Swin-T | SL-Swin-T | SL-Swin-T | SL-Swin-T | SL-Swin-T | SL-Swin-T\n",
    "| epochs | 20 (392 m) | 25 (531 m) | 30 (581 m) | 20 (382 m) | 30 (437 m) | 35 (499 m) | 50 (784 m)\n",
    "| batch_size | 32 | 32 | 32 | 48 | 48 | 48 | 48\n",
    "| learning_rate | 0.0005 | 0.0005 | 0.0005 | 0.0005 | 0.0005 | 0.0005 | 0.0005\n",
    "| True Positive | 14 | 17 | 11 | 10 | 11 | 11 | 11\n",
    "| False Positive | 354 | 363 | 360 | 366 | 348 | 357 | 380\n",
    "| False Negative | 43 | 40 | 46 | 47 | 46 | 46 | 46\n",
    "| Precision | 0.0380 | 0.0447 | 0.0296 | 0.0265 | 0.0306 | 0.0298 | 0.0281\n",
    "| Recall | 0.2456 | 0.2982 | 0.1929 | 0.1754 | 0.1929 | 0.1929 | 0.1929\n",
    "| F1-Score | 0.0658 | 0.0778 | 0.0514 | 0.0461 | 0.0528 | 0.0517 | 0.0491\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with pos_embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters | Value | Value | Value | Value | Value\n",
    "| --- | --- | --- | --- | --- | ---\n",
    "| model | SL-Swin-T | SL-Swin-T | SL-Swin-T | SL-Swin-T | SL-Swin-T\n",
    "| epochs | 25 (987 m) | 20 (389 m) | 25 (489 m) | 30 (611 m) | 25 (350 m)\n",
    "| batch_size | 16 | 32 | 32 | 32 | 48\n",
    "| learning_rate | 0.0005 | 0.0005 | 0.0005 | 0.0005 | 0.0005\n",
    "| True Positive | 11 | 12 | 16 | 13 | 11\n",
    "| False Positive | 325 | 342 | 370 | 335 | 371\n",
    "| False Negative | 46 | 45 | 41 | 44 | 46\n",
    "| Precision | 0.0327 | 0.0338 | 0.0414 | 0.0375 | 0.0287\n",
    "| Recall | 0.1929 | 0.2105 | 0.2807 | 0.2280 | 0.1929\n",
    "| F1-Score | 0.0559 | 0.0583 | 0.0722 | 0.0641 | 0.0501\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " p | TP | FP | FN | Precision | Recall | F1-Score\n",
      "0.01 | 35 | 4678 | 22 | 0.0074 | 0.6140 | 0.0147 |\n",
      "0.02 | 35 | 4454 | 22 | 0.0078 | 0.6140 | 0.0154 |\n",
      "0.03 | 35 | 4238 | 22 | 0.0082 | 0.6140 | 0.0162 |\n",
      "0.04 | 35 | 4038 | 22 | 0.0086 | 0.6140 | 0.0169 |\n",
      "0.05 | 33 | 3853 | 24 | 0.0085 | 0.5789 | 0.0167 |\n",
      "0.06 | 33 | 3654 | 24 | 0.0090 | 0.5789 | 0.0176 |\n",
      "0.07 | 32 | 3489 | 25 | 0.0091 | 0.5614 | 0.0179 |\n",
      "0.08 | 32 | 3328 | 25 | 0.0095 | 0.5614 | 0.0187 |\n",
      "0.09 | 31 | 3177 | 26 | 0.0097 | 0.5439 | 0.0190 |\n",
      "0.10 | 29 | 3013 | 28 | 0.0095 | 0.5088 | 0.0187 |\n",
      "0.11 | 29 | 2876 | 28 | 0.0100 | 0.5088 | 0.0196 |\n",
      "0.12 | 29 | 2728 | 28 | 0.0105 | 0.5088 | 0.0206 |\n",
      "0.13 | 29 | 2584 | 28 | 0.0111 | 0.5088 | 0.0217 |\n",
      "0.14 | 28 | 2467 | 29 | 0.0112 | 0.4912 | 0.0219 |\n",
      "0.15 | 28 | 2342 | 29 | 0.0118 | 0.4912 | 0.0231 |\n",
      "0.16 | 28 | 2227 | 29 | 0.0124 | 0.4912 | 0.0242 |\n",
      "0.17 | 27 | 2113 | 30 | 0.0126 | 0.4737 | 0.0246 |\n",
      "0.18 | 26 | 2015 | 31 | 0.0127 | 0.4561 | 0.0248 |\n",
      "0.19 | 24 | 1918 | 33 | 0.0124 | 0.4211 | 0.0240 |\n",
      "0.20 | 24 | 1818 | 33 | 0.0130 | 0.4211 | 0.0253 |\n",
      "0.21 | 24 | 1723 | 33 | 0.0137 | 0.4211 | 0.0266 |\n",
      "0.22 | 24 | 1640 | 33 | 0.0144 | 0.4211 | 0.0279 |\n",
      "0.23 | 24 | 1562 | 33 | 0.0151 | 0.4211 | 0.0292 |\n",
      "0.24 | 22 | 1485 | 35 | 0.0146 | 0.3860 | 0.0281 |\n",
      "0.25 | 22 | 1420 | 35 | 0.0153 | 0.3860 | 0.0294 |\n",
      "0.26 | 21 | 1353 | 36 | 0.0153 | 0.3684 | 0.0294 |\n",
      "0.27 | 21 | 1276 | 36 | 0.0162 | 0.3684 | 0.0310 |\n",
      "0.28 | 21 | 1220 | 36 | 0.0169 | 0.3684 | 0.0324 |\n",
      "0.29 | 20 | 1163 | 37 | 0.0169 | 0.3509 | 0.0323 |\n",
      "0.30 | 20 | 1101 | 37 | 0.0178 | 0.3509 | 0.0340 |\n",
      "0.31 | 19 | 1041 | 38 | 0.0179 | 0.3333 | 0.0340 |\n",
      "0.32 | 19 | 1000 | 38 | 0.0186 | 0.3333 | 0.0353 |\n",
      "0.33 | 19 | 952 | 38 | 0.0196 | 0.3333 | 0.0370 |\n",
      "0.34 | 19 | 897 | 38 | 0.0207 | 0.3333 | 0.0391 |\n",
      "0.35 | 19 | 855 | 38 | 0.0217 | 0.3333 | 0.0408 |\n",
      "0.36 | 19 | 820 | 38 | 0.0226 | 0.3333 | 0.0424 |\n",
      "0.37 | 17 | 787 | 40 | 0.0211 | 0.2982 | 0.0395 |\n",
      "0.38 | 17 | 748 | 40 | 0.0222 | 0.2982 | 0.0414 |\n",
      "0.39 | 16 | 701 | 41 | 0.0223 | 0.2807 | 0.0413 |\n",
      "0.40 | 16 | 654 | 41 | 0.0239 | 0.2807 | 0.0440 |\n",
      "0.41 | 16 | 620 | 41 | 0.0252 | 0.2807 | 0.0462 |\n",
      "0.42 | 16 | 592 | 41 | 0.0263 | 0.2807 | 0.0481 |\n",
      "0.43 | 16 | 563 | 41 | 0.0276 | 0.2807 | 0.0503 |\n",
      "0.44 | 15 | 536 | 42 | 0.0272 | 0.2632 | 0.0493 |\n",
      "0.45 | 13 | 512 | 44 | 0.0248 | 0.2281 | 0.0447 |\n",
      "0.46 | 13 | 484 | 44 | 0.0262 | 0.2281 | 0.0469 |\n",
      "0.47 | 13 | 457 | 44 | 0.0277 | 0.2281 | 0.0493 |\n",
      "0.48 | 13 | 441 | 44 | 0.0286 | 0.2281 | 0.0509 |\n",
      "0.49 | 13 | 421 | 44 | 0.0300 | 0.2281 | 0.0530 |\n",
      "0.50 | 12 | 396 | 45 | 0.0294 | 0.2105 | 0.0516 |\n",
      "0.51 | 12 | 372 | 45 | 0.0313 | 0.2105 | 0.0544 |\n",
      "0.52 | 12 | 348 | 45 | 0.0333 | 0.2105 | 0.0576 |\n",
      "0.53 | 12 | 328 | 45 | 0.0353 | 0.2105 | 0.0605 |\n",
      "0.54 | 12 | 311 | 45 | 0.0372 | 0.2105 | 0.0632 |\n",
      "0.55 | 12 | 292 | 45 | 0.0395 | 0.2105 | 0.0665 |\n",
      "0.56 | 12 | 273 | 45 | 0.0421 | 0.2105 | 0.0702 |\n",
      "0.57 | 12 | 259 | 45 | 0.0443 | 0.2105 | 0.0732 |\n",
      "0.58 | 12 | 249 | 45 | 0.0460 | 0.2105 | 0.0755 |\n",
      "0.59 | 12 | 234 | 45 | 0.0488 | 0.2105 | 0.0792 |\n",
      "0.60 | 12 | 220 | 45 | 0.0517 | 0.2105 | 0.0830 |\n",
      "0.61 | 12 | 215 | 45 | 0.0529 | 0.2105 | 0.0845 |\n",
      "0.62 | 11 | 206 | 46 | 0.0507 | 0.1930 | 0.0803 |\n",
      "0.63 | 11 | 192 | 46 | 0.0542 | 0.1930 | 0.0846 |\n",
      "0.64 | 9 | 183 | 48 | 0.0469 | 0.1579 | 0.0723 |\n",
      "0.65 | 9 | 170 | 48 | 0.0503 | 0.1579 | 0.0763 |\n",
      "0.66 | 9 | 164 | 48 | 0.0520 | 0.1579 | 0.0783 |\n",
      "0.67 | 9 | 157 | 48 | 0.0542 | 0.1579 | 0.0807 |\n",
      "0.68 | 9 | 147 | 48 | 0.0577 | 0.1579 | 0.0845 |\n",
      "0.69 | 9 | 138 | 48 | 0.0612 | 0.1579 | 0.0882 |\n",
      "0.70 | 8 | 135 | 49 | 0.0559 | 0.1404 | 0.0800 |\n",
      "0.71 | 7 | 129 | 50 | 0.0515 | 0.1228 | 0.0725 |\n",
      "0.72 | 7 | 122 | 50 | 0.0543 | 0.1228 | 0.0753 |\n",
      "0.73 | 7 | 116 | 50 | 0.0569 | 0.1228 | 0.0778 |\n",
      "0.74 | 6 | 110 | 51 | 0.0517 | 0.1053 | 0.0694 |\n",
      "0.75 | 6 | 101 | 51 | 0.0561 | 0.1053 | 0.0732 |\n",
      "0.76 | 6 | 95 | 51 | 0.0594 | 0.1053 | 0.0759 |\n",
      "0.77 | 6 | 90 | 51 | 0.0625 | 0.1053 | 0.0784 |\n",
      "0.78 | 6 | 85 | 51 | 0.0659 | 0.1053 | 0.0811 |\n",
      "0.79 | 6 | 81 | 51 | 0.0690 | 0.1053 | 0.0833 |\n",
      "0.80 | 6 | 76 | 51 | 0.0732 | 0.1053 | 0.0863 |\n",
      "0.81 | 6 | 74 | 51 | 0.0750 | 0.1053 | 0.0876 |\n",
      "0.82 | 6 | 68 | 51 | 0.0811 | 0.1053 | 0.0916 |\n",
      "0.83 | 6 | 62 | 51 | 0.0882 | 0.1053 | 0.0960 |\n",
      "0.84 | 6 | 60 | 51 | 0.0909 | 0.1053 | 0.0976 |\n",
      "0.85 | 6 | 58 | 51 | 0.0938 | 0.1053 | 0.0992 |\n",
      "0.86 | 6 | 57 | 51 | 0.0952 | 0.1053 | 0.1000 |\n",
      "0.87 | 6 | 57 | 51 | 0.0952 | 0.1053 | 0.1000 |\n",
      "0.88 | 6 | 55 | 51 | 0.0984 | 0.1053 | 0.1017 |\n",
      "0.89 | 6 | 53 | 51 | 0.1017 | 0.1053 | 0.1034 |\n",
      "0.90 | 5 | 52 | 52 | 0.0877 | 0.0877 | 0.0877 |\n",
      "0.91 | 5 | 50 | 52 | 0.0909 | 0.0877 | 0.0893 |\n",
      "0.92 | 5 | 48 | 52 | 0.0943 | 0.0877 | 0.0909 |\n",
      "0.93 | 4 | 46 | 53 | 0.0800 | 0.0702 | 0.0748 |\n",
      "0.94 | 4 | 44 | 53 | 0.0833 | 0.0702 | 0.0762 |\n",
      "0.95 | 4 | 41 | 53 | 0.0889 | 0.0702 | 0.0784 |\n",
      "0.96 | 4 | 40 | 53 | 0.0909 | 0.0702 | 0.0792 |\n",
      "0.97 | 3 | 38 | 54 | 0.0732 | 0.0526 | 0.0612 |\n",
      "0.98 | 3 | 36 | 54 | 0.0769 | 0.0526 | 0.0625 |\n",
      "0.99 | 3 | 32 | 54 | 0.0857 | 0.0526 | 0.0652 |\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "\n",
    "ablation_dict = ablation_study_p_dev(\n",
    "    preds,\n",
    "    clean_subjects_videos_ground_truth_labels,\n",
    "    clean_videos_images,\n",
    "    clean_subjects,\n",
    "    clean_subjects_videos_code,\n",
    "    k,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6696e3028ae55fa5be357812587f73779dfb157cc851dab91c4ca8ffd3c7a806"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
